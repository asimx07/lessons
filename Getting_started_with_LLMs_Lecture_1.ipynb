{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x-2_sbJy5AAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Large Language Models (LLMs) and OpenAI GPT API\n",
        "## Overview\n",
        "In this notebook, we'll explore Large Language Models (LLMs) with a focus on OpenAI's GPT models. You'll learn how to interact with GPT models using Python, make API requests, and handle responses.\n",
        "\n",
        "### Objectives:- Understand the basics of LLMs and their applications.\n",
        "- Set up your environment to interact with the OpenAI API.\n",
        "- Learn how to obtain and securely store an OpenAI API key.\n",
        "- Make your first API requests to GPT-3.5 and analyze the responses.\n",
        "\n",
        "Let's get started!\n"
      ],
      "metadata": {
        "id": "qQmpqcF9AmZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Large Language Models (LLMs)?\n",
        "\n",
        "Large Language Models (LLMs) are a type of artificial intelligence model designed to understand and generate human-like text. They are trained on vast amounts of text data and can perform a variety of tasks such as:\n",
        "\n",
        "- Text generation\n",
        "- Translation\n",
        "- Summarization\n",
        "- Question answering\n",
        "\n",
        "### What is GPT?\n",
        "\n",
        "GPT stands for Generative Pre-trained Transformer, a type of artificial intelligence model developed by OpenAI. It's designed to understand and generate human-like text based on the input it receives. Here’s a breakdown:\n",
        "\n",
        "Generative: GPT can generate new content. Unlike some models that simply classify or predict, GPT can produce coherent and contextually relevant text on a wide range of topics.\n",
        "\n",
        "Pre-trained: Before GPT is fine-tuned for specific tasks, it is first pre-trained on a vast amount of text data from the internet. This pre-training helps the model learn the structure of language, including grammar, facts about the world, and some reasoning abilities.\n",
        "\n",
        "Transformer: The Transformer architecture is the backbone of GPT. It uses attention mechanisms to focus on different parts of the input data, allowing it to handle long-range dependencies in text. This architecture enables GPT to process and generate text more effectively than previous models.\n",
        "\n",
        "### Use Cases of LLMs-\n",
        "**Content Creation:** Generate articles, blog posts, or creative writing.\n",
        "-**Customer Support:** Build chatbots that can handle customer inquiries.\n",
        "-**Coding Assistance:** Help with code generation and debugging.\n",
        "\n",
        "### What are Tokens?\n",
        "In the context of language models like GPT, tokens are the building blocks of text that the model processes. They can represent individual words, subwords, characters, or punctuation marks. Here’s a detailed explanation:\n",
        "\n",
        "#### What Tokens Represent:\n",
        "\n",
        "- Words: Common words are often treated as single tokens. For example, the word \"hello\" would be a single token.\n",
        "Subwords: For less common or complex words, the model might break them down into smaller subword tokens. For instance, the word \"unhappiness\" might be split into \"un\", \"happi\", and \"ness\".\n",
        "Characters: In some cases, individual characters might be used as tokens, especially for unusual or non-standard text.\n",
        "How Tokens Work:\n",
        "\n",
        "- When you input text into a GPT model, it first converts the text into a sequence of tokens.\n",
        "The model then processes these tokens to understand the context and generate responses.\n",
        "The output generated by the model is also in the form of tokens, which are then converted back into human-readable text.\n",
        "Tokenization Process:\n",
        "\n",
        "- Tokenization is the process of breaking down text into tokens. Different tokenization methods can be used depending on the language model.\n",
        "For example, \"I love AI\" might be tokenized into three tokens: [\"I\", \"love\", \"AI\"].\n",
        "Example: https://gpt-tokenizer.dev/\n",
        "Importance of Tokens:\n",
        "\n",
        "- The number of tokens in a piece of text is important because it affects the model’s performance and cost. GPT models have a limit on how many tokens they can process at once.\n",
        "Pricing for using GPT-based APIs, like OpenAI’s, often depends on the number of tokens processed, both for the input and the output.\n",
        "Token Limit:\n",
        "\n",
        "- Each GPT model has a maximum token limit. For example, GPT-3 has a limit of 4,096 tokens in a single request. This includes both the input and the generated output.\n",
        "- If your input text is too long, it might get truncated or require multiple requests.\n",
        "- Understanding tokens is crucial when working with language models, especially for optimizing performance and managing costs.\n",
        "\n",
        "\n",
        "Now, let's set up our environment to start working with GPT Model!\n"
      ],
      "metadata": {
        "id": "0LL_u9lFAq2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Your Environment\n",
        "\n",
        "Before we can start interacting with LLMs, we need to set up our Python environment. This involves installing necessary packages and creating a Jupyter Notebook to run our code.\n",
        "\n",
        "### Step 1: Install Required Packages\n",
        "We need to install the `openai` package to interact with the OpenAI API.\n",
        "\n",
        "You can install the required packages by running the following command:\n"
      ],
      "metadata": {
        "id": "rF-13GISA82w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ne0vmVea4m3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2143ac26-2f85-4bb4-bcf6-95d7468f9ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.41.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.41.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.4/362.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 openai-1.41.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtaining and Securing Your OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you'll need an API key. Here's how you can obtain one:\n",
        "\n",
        "### Step 1: Sign Up for OpenAI- Go to the [OpenAI website](https://platform.openai.com/signup) and sign up for an account.\n",
        "- Once signed up, navigate to the API keys section in your account dashboard.\n",
        "\n",
        "### Step 2: Generate an API Key- Click on \"Create new secret key\" to generate an API key.\n",
        "- Copy this key and store it securely. **Do not share this key publicly**.\n",
        "\n",
        "### Step 3: Securely Store Your API Key\n",
        "It's important to store your API key securely. One common method is to use environment variables. Here's how:\n",
        "\n",
        "- Create an environment variable in your operating system to store the API key.\n",
        "- Access the API key in your Python code using the `os` library.\n",
        "\n",
        "Let's see how to do this in Python.\n"
      ],
      "metadata": {
        "id": "vd6c5egmBQx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Option 1: Store your API key in an environment variable for security\n",
        "#API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Option 2: Set the API key directly (not recommended for production)\n",
        "\n",
        "API_KEY = \"sk-xxxxxx.....\" # Replace with your actual API key"
      ],
      "metadata": {
        "id": "P9Ze9WaQBhtR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Your First API Request to LLM\n",
        "\n",
        "With your API key set up, you're now ready to make your first request to GPT-4o-mini. The process involves sending a prompt to the model and receiving a text-based response.\n",
        "\n",
        "First of all you need to instantiate OpenAI client with you API KEY"
      ],
      "metadata": {
        "id": "Lt1b8pLdB2_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "tBpO166C7nnU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lets start with examples now"
      ],
      "metadata": {
        "id": "uEStKyG1L2oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smT6zBDiCbK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Simple chat completion\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me capital of Pakistan\"}\n",
        "  ]\n",
        ")\n",
        "#print(completion)\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU2aBFax6aLd",
        "outputId": "1e624b1c-5543-41f1-edb3-f2e5d7b1694d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Pakistan is Islamabad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Example 2: Adjusting API Parameters\n",
        "\n",
        "#You can customize the behavior of LLM by adjusting parameters like `temperature`, `max_tokens`. The `temperature` parameter controls the randomness of the output\n",
        "#where a higher value (up to 1) makes the output more creative, and a lower value (closer to 0) makes it more deterministic.\n",
        "\n",
        "#Let's try an example where we adjust these parameters.\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a creative story teller\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me a story about Mirza Ghalib\"}\n",
        "  ],\n",
        "  max_tokens=500,\n",
        "  temperature=0.9\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSWVosse6HQn",
        "outputId": "3778dac2-f8a9-4264-d41d-14dbe2239a2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in the bustling heart of Delhi, during the 19th century, there lived a poet whose words could weave magic into the air around him. His name was Mirza Asadullah Khan Ghalib, a man as enigmatic as the verses he penned. The streets of Chandni Chowk were alive with the sounds of merchants haggling, the aroma of spices wafting through the air, and the distant sounds of music from the courtyards of noble homes. But in the quiet corners of the city, a different kind of magic unfolded—the kind that filled the soul with longing, love, and a profound sense of loss.\n",
            "\n",
            "Ghalib's life was a tapestry woven with threads of joy, heartache, and unfulfilled dreams. He was a man of contradictions, often juggling the burdens of financial strife and the weight of his genius. His home was filled with the soft glow of oil lamps, where he sat, scribbling verses on scraps of paper—a reflection of his restless spirit. The world around him was constantly changing, but the poetry that flowed from his heart remained timeless.\n",
            "\n",
            "One winter evening, as a chill crept into the air, Ghalib found himself in the company of friends, gathered around a hookah in a dimly lit room. The flicker of candlelight danced on the walls, casting shadows like fleeting thoughts. His dear friend, the poet and philosopher, asked, \"Ghalib, what is it that stirs your heart so deeply?\" \n",
            "\n",
            "With a mischievous twinkle in his eye, Ghalib replied, \"Ah, my friend, I am but a wanderer in this world, forever searching for a companion in the pages of my poetry. Love, in its most elusive form, calls to me, and pain holds my hand as we traverse the endless journey of existence.\"\n",
            "\n",
            "As night deepened, a conversation about the nature of love ensued. Ghalib spoke with a passion that had his friends entranced. \"Love is a tempest,\" he said, \"a flame that burns with both ecstasy and agony. It is the very essence of life. Without it, we float like dust in the wind, aimless and lost.\"\n",
            "\n",
            "Among those gathered was a young woman named Nasrin, a skilled musician with a voice that could silence the chaos of the world. She listened with rapt attention, her heart resonating with Ghalib's words. Perhaps it was her admiration\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What are the key differences between Python and Javascript?\"\n",
        "\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ],\n",
        "  max_tokens=100,\n",
        "  temperature=0.1\n",
        ")\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWvgyDacEOLz",
        "outputId": "02848ce3-724a-45d7-d3ca-db0a84eb1459"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Syntax: Python uses indentation to define code blocks, while JavaScript uses curly braces {}.\n",
            "\n",
            "2. Typing: Python is a dynamically typed language, meaning that variable types are determined at runtime. JavaScript is a loosely typed language, meaning that variables can change types during execution.\n",
            "\n",
            "3. Use cases: Python is often used for data analysis, machine learning, and scientific computing, while JavaScript is primarily used for web development.\n",
            "\n",
            "4. Libraries and frameworks: Python has a wide range of libraries and frameworks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.list()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCFZ9VvFElNl",
        "outputId": "44b34ca9-48dd-4416-8f90-b0daebb33d93"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'), Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a function to remove redundancy in the code."
      ],
      "metadata": {
        "id": "0yU8Esb6Fujn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt,max_output_token,llm_temperature):\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_output_token,\n",
        "        temperature=llm_temperature\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "\n",
        "prompt = \"What are the key differences between Python and Java?\"\n",
        "print(get_completion(prompt,200,0.3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xUg8mA-F0sT",
        "outputId": "9f66dc0b-d2f7-448d-82f6-0345b4174e0a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python and Java are both popular programming languages, but they have several key differences that make them suitable for different types of projects and preferences. Here are some of the main distinctions:\n",
            "\n",
            "### 1. **Syntax and Readability**\n",
            "- **Python**: Known for its clean and readable syntax. It uses indentation to define code blocks, which promotes readability and reduces the need for braces or semicolons.\n",
            "- **Java**: Has a more verbose syntax that requires explicit declaration of data types and the use of braces to define code blocks. This can make Java code longer and sometimes less readable than Python.\n",
            "\n",
            "### 2. **Typing System**\n",
            "- **Python**: Dynamically typed, meaning that variable types are determined at runtime. This allows for more flexibility but can lead to runtime errors if types are not managed carefully.\n",
            "- **Java**: Statically typed, requiring explicit declaration of variable types at compile time. This can help catch type-related errors early in the development process.\n",
            "\n",
            "### 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_with_system_message(system_message,prompt,max_output_token,llm_temperature):\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_output_token,\n",
        "        temperature=llm_temperature\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "system_message = \"Just give me one word answer\"\n",
        "prompt = \"Who has the most ducks in ODI cricket?\"\n",
        "print(get_completion_with_system_message(system_message,prompt,200,0.3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCbGjp8SGjaa",
        "outputId": "515c3156-aeb2-4357-a4e2-a41cd27bbfc3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jayasuriya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_with_history(system_message,prompt,max_output_token,llm_temperature):\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_output_token,\n",
        "        temperature=llm_temperature\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "system_message = \"You are helpful AI chat Assistant. \"\n",
        "prompt = \"Who has the most ducks in ODI cricket?\"\n",
        "result = get_completion_with_history(system_message,prompt,200,0.1)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QemWKVhQHyH8",
        "outputId": "c6496414-22f9-4ccc-f446-5e7d846112bb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last update in October 2023, the record for the most ducks in One Day International (ODI) cricket is held by Sri Lankan cricketer Sanath Jayasuriya. He accumulated 34 ducks during his ODI career. Please verify with the latest statistics as records in sports can change frequently.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Who has the second most?\"\n",
        "\n",
        "context = f\"{prompt}\\n{result}\"\n",
        "#print(context)\n",
        "result = get_completion_with_history(system_message,context,200,0.1)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yG6YC5dJHdM",
        "outputId": "5107c959-eddf-4890-e1f6-84b36975c791"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As of my last update in October 2023, Shahid Afridi of Pakistan holds the second-most ducks in One Day International (ODI) cricket with 30 ducks, following Sanath Jayasuriya who has 34 ducks. \n",
            "\n",
            "Cricket statistics are dynamic and can change as players continue to participate in matches. For the most current and accurate information, it's advisable to refer to reliable cricket databases or sports news sources such as ESPN Cricinfo or the official ICC website.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Exercise: Building Your Own Prompt-Response System\n",
        "\n",
        "Now that you've seen how to interact with LLM, it's time to build your own simple application.\n",
        "\n",
        "### Task:- Create a Jupyter Notebook script that:\n",
        "  1. Connects to the OPEN AI API.\n",
        "  2. Accepts a user-defined prompt.\n",
        "  3. Sends the prompt to LLM and returns the response.\n",
        "\n",
        "### Hints:- Experiment with different prompts to see how LLM responds.\n",
        "- Adjust the `max_tokens` and `temperature` parameters to observe how they affect the output.\n",
        "- Consider building a simple loop that allows the user to keep entering prompts without restarting the kernel.\n"
      ],
      "metadata": {
        "id": "7tRQdGLSFd62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = ''\n",
        "result = ''\n",
        "system_message = \"You are helpful AI chat Assistant. \"\n",
        "while True:\n",
        "    user_prompt = input(\"Enter a prompt for AI (or 'exit' to quit): \")\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        break\n",
        "    context= f\"{user_prompt}\\n{result}\"\n",
        "    result = get_completion_with_history(system_message,context,150,0.5)\n",
        "    print(\"AI:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb1UyC00Fa9K",
        "outputId": "0ee6e173-8bff-4412-aae4-ff5ab9311a46"
      },
      "execution_count": 41,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter a prompt for AI (or 'exit' to quit): What is Capital of Saudi Arabia\n",
            "AI: The capital of Saudi Arabia is Riyadh.\n",
            "Enter a prompt for AI (or 'exit' to quit): What's its population\n",
            "AI: As of the latest available data in 2023, Riyadh, the capital of Saudi Arabia, has an estimated population of around 7.7 million people. Keep in mind that population figures can change, so it's always a good idea to check the most recent statistics for the most accurate information.\n",
            "Enter a prompt for AI (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsAI1DMxOvX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}